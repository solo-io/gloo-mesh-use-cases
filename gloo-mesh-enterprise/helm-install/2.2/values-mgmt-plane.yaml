######################################################################################
# This is an example Helm values file for PRODUCTION-LEVEL installations.            #
# You MUST edit some settings in this file to provide your own details.              #
# To see the default settings instead, run                                           #
# helm show values gloo-mesh-enterprise/gloo-mesh-enterprise --version $GLOO_VERSION #
######################################################################################

# Global settings for management plane configuration
# Namespace to install Gloo Platform components
adminNamespace: gloo-mesh
# Set the logger to development mode, which can cause panics. Do not use in production.
devMode: false
# Name of the management cluster
global:
  cluster: $MGMT_CLUSTER
# Set to true to permit unencrypted and unauthenticated communication between management plane and data planes. Do not use in production.
insecure: false
# Enable leader election for the HA deployment
leaderElection: true
# Name of the management cluster
mgmtClusterName: $MGMT_CLUSTER
# Do not install the Gloo agent into the management cluster in a multicluster setup
registerMgmtPlane:
  enabled: false
verbose: false

# License settings
# Provide your $ GLOO_GATEWAY_LICENSE_KEY to also install Gloo Gateway
glooGatewayLicenseKey: ""
# Your Gloo Mesh Enterprise license that you got from your Solo account representative
glooMeshLicenseKey: $GLOO_MESH_LICENSE_KEY
# Provide your $ GLOO_NETWORK_LICENSE_KEY to also install Gloo Network
glooNetworkLicenseKey: ""
# Provide your $ GLOO_TRIAL_LICENSE_KEY for a trial installation of all products
glooTrialLicenseKey: ""
# To instead provide license keys in a secret in the gloo-mesh namespace of the management cluster, specify the secret name.
licenseSecretName: ""

# Enable the default Prometheus server, and deploy a production-level server to scrape
# metrics from this server. If needed, disable this default Prometheus instance
# to provide your own.
prometheus:
  alertmanager:
    enabled: false
  enabled: true
  kubeStateMetrics:
    enabled: false
  nodeExporter:
    enabled: false
  podSecurityPolicy:
    enabled: false
  pushgateway:
    enabled: false
  rbac:
    create: true
  server:
    fullnameOverride: prometheus-server
    persistentVolume:
      enabled: false
  serverFiles:
    alerting_rules.yml:
      groups:
      - name: GlooPlatformAlerts
        rules:
        - alert: GlooPlatformTranslationLatencyIsHigh
          annotations:
            runbook: "https://docs.solo.io/gloo-mesh-enterprise/latest/troubleshooting/gloo/"
            summary: "The translation time has increased above 5 sec. It's currently {{ $value | humanize }}."
          expr: histogram_quantile(0.99, sum(rate(gloo_mesh_translation_time_sec_bucket[5m])) by(le)) > 5
          for: 15m
          labels:
            severity: warning
        - alert: GlooPlatformReconscilerLatencyIsHigh
          annotations:
            runbook: "https://docs.solo.io/gloo-mesh-enterprise/latest/troubleshooting/gloo/"
            summary: "The reconciliation time has increased above 3 sec. It's currently {{ $value | humanize }}."
          expr: histogram_quantile(0.99, sum(rate(gloo_mesh_reconciler_time_sec_bucket[5m])) by(le)) > 3
          for: 15m
          labels:
            severity: warning
        - alert: GlooPlatformAgentsAreDisconnected
          annotations:
            runbook: "https://docs.solo.io/gloo-mesh-enterprise/latest/troubleshooting/gloo/"
            summary: "The following cluster is disconnected: {{ $labels.cluster }}. Check the Gloo Platform Agent pod in the cluster!"
          expr: count by(cluster) (sum by(cluster) (relay_push_clients_warmed == 0)) > 0
          for: 5m
          labels:
            severity: warning
        - alert: GlooPlatformTranslationWarnings
          annotations:
            runbook: "https://docs.solo.io/gloo-mesh-enterprise/latest/troubleshooting/gloo/"
            summary: "Gloo Platform has detected {{$value | humanize}} translation warnings in the last 5m. Check your {{ $labels.gvk }} resources!"
          expr: increase(translation_warning[5m]) > 0
          labels:
            severity: warning
        - alert: GlooPlatformTranslationErrors
          annotations:
            runbook: "https://docs.solo.io/gloo-mesh-enterprise/latest/troubleshooting/gloo/"
            summary: "Gloo Platform has detected {{$value | humanize}} translation errors in the last 5m. Check your {{ $labels.gvk }} resources!"
          expr: increase(translation_error[5m]) > 0
          labels:
            severity: warning
        - alert: GlooPlatformRedisErrors
          annotations:
            runbook: "https://docs.solo.io/gloo-mesh-enterprise/latest/troubleshooting/gloo/"
            summary: "Gloo Platform has detected {{$value | humanize}} Redis sync errors in the last 5m."
          expr: increase(gloo_mesh_redis_sync_err[5m]) > 0
          labels:
            severity: warning
    prometheus.yml:
      scrape_configs:
      - job_name: gloo-platform-pods
        kubernetes_sd_configs:
        - namespaces:
            names:
            - gloo-mesh
          role: pod
        relabel_configs:
        - action: keep
          regex: true
          source_labels:
          - __meta_kubernetes_pod_annotation_prometheus_io_scrape
        - action: replace
          regex: (.+)
          source_labels:
          - __meta_kubernetes_pod_annotation_prometheus_io_path
          target_label: __metrics_path__
        - action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          source_labels:
          - __address__
          - __meta_kubernetes_pod_annotation_prometheus_io_port
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - action: replace
          source_labels:
          - __meta_kubernetes_namespace
          target_label: namespace
        - action: replace
          source_labels:
          - __meta_kubernetes_pod_name
          target_label: pod
        - action: keep
          regex: gloo-mesh-mgmt-server
          source_labels:
          - __meta_kubernetes_pod_label_app
        scrape_interval: 15s
        scrape_timeout: 10s
  serviceAccounts:
    alertmanager:
      create: false
    nodeExporter:
      create: false
    pushgateway:
      create: false
    server:
      create: true
# Default Prometheus server address. Replace with your own as needed.
prometheusUrl: http://prometheus-server

# Configuration for the gloo-mesh-mgmt-server deployment
glooMeshMgmtServer:
  enabled: true
  env:
  - name: POD_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace
  - name: LICENSE_KEY
    valueFrom:
      secretKeyRef:
        key: key
        name: gloo-mesh-enterprise-license
        optional: true
  # Required for OpenShift installations: Allow the pod to be assigned a dynamic user ID.
  floatingUserId: false
  image:
    pullPolicy: IfNotPresent
    registry: gcr.io/gloo-mesh
    repository: gloo-mesh-mgmt-server
    # Gloo Mesh Enterprise patch version. For a list of patch versions,
    # see https://docs.solo.io/gloo-mesh-enterprise/main/reference/changelog/
    tag: $GLOO_VERSION
    # If pulling from a private registry
    #pullSecret:
  ports:
    # gloo-mesh-mgmt-server service port for Gloo agents to connect to
    grpc: 9900
    healthcheck: 8090
  # gloo-mesh-mgmt-server pod resources
  resources:
    requests:
      cpu: 125m
      memory: 256Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  # Static user ID to run the containers as. Unused if floatingUserId is 'true'
  runAsUser: 10101
  # Additional settings to add to the load balancer service
  serviceOverrides:
    metadata:
      annotations:
        # AWS-specific annotations
        service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: "2"
        service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: "2"
        service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "10"
        service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: "9900"
        service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: "tcp"

        service.beta.kubernetes.io/aws-load-balancer-type: external
        service.beta.kubernetes.io/aws-load-balancer-scheme: internal
        service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
        service.beta.kubernetes.io/aws-load-balancer-backend-protocol: TCP
        service.beta.kubernetes.io/aws-load-balancer-private-ipv4-addresses: 10.0.50.50, 10.0.64.50
        service.beta.kubernetes.io/aws-load-balancer-subnets: subnet-0478784f04c486de5, subnet-09d0cf74c0117fcf3
        service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: deregistration_delay.connection_termination.enabled=true,deregistration_delay.timeout_seconds=1
  # Kubernetes load balancer service type
  serviceType: LoadBalancer
  # Optional configuration for the deployed containers
  sidecars: {}
  # Concurrency to use for translation operations
  concurrency: 10
  # Allows mgmt-server replicas to auto-balance the number of connected clusters based on the number of replicas and total clusters.
  # Experimental; do not use in production.
  enableClusterLoadBalancing: false
  # Port on which to serve internal Prometheus metrics for the management server app
  statsPort: 9091
  # Maximum message size for gRPC messages sent and received by the management server
  maxGrpcMessageSize: "4294967295"
  # Configuration for certificates to secure server-agent relay communication
  # IMPORTANT: These settings assume you are bringing your own custom certificates.
  # To use default certificates instead, see the commented-out relay section below.
  relay:
    # Disable default relay CA functionality only when supplying custom client certs to the agents for relay mTLS
    disableCa: true
    disableCaCertGeneration: true
    disableTokenGeneration: true
    # Push RBAC resources to the management server. Required for multicluster RBAC in the UI
    pushRbac: true
    signingTlsSecret:
      name: relay-tls-signing-secret
    # Custom certs: Reference to a secret containing TLS certificates used to secure the networking gRPC server with TLS
    tlsSecret:
      name: relay-server-tls-secret
    # (NOT needed with ACM certs) Secret containing a shared token for authenticating relay agents when they first communicate with the management plane
    #tokenSecret:
      #key: token
      #name: relay-identity-token-secret
      #namespace: gloo-mesh
  # To use default certificates instead of custom, you can comment out the above section and uncomment this section:
  #relay:
  #  disableCa: false
  #  disableCaCertGeneration: false
  #  disableTokenGeneration: false
  #  pushRbac: true
  #  signingTlsSecret:
  #    name: relay-tls-signing-secret
  #  tlsSecret:
  #    name: relay-server-tls-secret
  #  tokenSecret:
  #    key: token
  #    name: relay-identity-token-secret
  #    namespace: gloo-mesh

# Configuration for the Gloo UI
glooMeshUi:
  enabled: true
  env:
  - name: POD_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace
  - name: LICENSE_KEY
    valueFrom:
      secretKeyRef:
        key: key
        name: gloo-mesh-enterprise-license
        optional: true
  # Required for OpenShift installations: Allow the pod to be assigned a dynamic user ID.
  floatingUserId: false
  image:
    pullPolicy: IfNotPresent
    registry: gcr.io/gloo-mesh
    repository: gloo-mesh-apiserver
    # Gloo Mesh Enterprise patch version
    tag: $GLOO_VERSION
    # If pulling from a private registry
    #pullSecret:
  # Ports for UI service
  ports:
    console: 8090
    grpc: 10101
    healthcheck: 8081
  # UI pod resources
  resources:
    requests:
      cpu: 125m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Gi
  # Static user ID to run the containers as. Unused if floatingUserId is 'true'
  runAsUser: 10101
  # Additional settings to add to Kubernetes service
  #serviceOverrides:
  # Kubernetes service type
  serviceType: ClusterIP
  # Configuration for the console and envoy containers
  sidecars:
    console:
      env: null
      image:
        pullPolicy: IfNotPresent
        registry: gcr.io/gloo-mesh
        repository: gloo-mesh-ui
        # Gloo Mesh Enterprise patch version
        tag: $GLOO_VERSION
        # If pulling from a private registry
        #pullSecret:
      resources:
        requests:
          cpu: 125m
          memory: 256Mi
    envoy:
      env:
      - name: ENVOY_UID
        value: "0"
      image:
        pullPolicy: IfNotPresent
        registry: gcr.io/gloo-mesh
        repository: gloo-mesh-envoy
        # Gloo Mesh Enterprise patch version
        tag: $GLOO_VERSION
        # If pulling from a private registry
        #pullSecret:
      resources:
        requests:
          cpu: 500m
          memory: 256Mi
  # Configure OIDC authentication for the UI
  auth:
    backend: oidc
    # Disabled by default. Enable to provide your OIDC auth details
    enabled: false
    oidc:
      # URL that UI for OIDC app is available at, from the DNS and other ingress settings that expose OIDC app UI service
      appUrl: ""
      # From the OIDC provider
      clientId: ""
      # From the OIDC provider, stored in a secret
      clientSecret: ""
      clientSecretName: dashboard
      # Issuer URL from the OIDC provider, such as https://<domain>.<provider_url>/
      issuerUrl: ""
      # Redis instance configuration, optionally used for auth session storage
      session:
        backend: redis
        redis:
          # Point to your own Redis instance as needed
          host: ""
  # To use the license keys in your provided secret instead of referencing the
  # secret generated by this Helm chart, specify the secret name.
  licenseSecretName: ""
  # Name of the dashboard settings object to use
  settingsName: settings

# Configuration for an optional Redis instance to store ID tokens for UI auth
glooMeshRedis:
  # As needed, disable default instance to provide your own Redis instance
  enabled: true
  env:
  - name: MASTER
    value: "true"
  # Required for OpenShift installations: Allow the pod to be assigned a dynamic user ID.
  floatingUserId: false
  image:
    pullPolicy: IfNotPresent
    registry: docker.io
    repository: redis
    tag: 7.0.4
  ports:
    redis: 6379
  resources:
    requests:
      cpu: 125m
      memory: 256Mi
  # Static user ID to run the containers as. Unused if floatingUserId is 'true'
  runAsUser: 10101
  serviceType: ClusterIP
  sidecars: {}
  addr: ""